{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistants API - Knowledge Retrievel\n",
    "\n",
    "Retrieval augments the Assistant with knowledge from outside its model, such as proprietary product information or documents provided by your users. Once a file is uploaded and passed to the Assistant, OpenAI will automatically chunk your documents, index and store the embeddings, and implement vector search to retrieve relevant content to answer user queries.\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\n",
    "\n",
    "#### How it works\n",
    "The model then decides when to retrieve content based on the user Messages. The Assistants API automatically chooses between two retrieval techniques:\n",
    "\n",
    "    1. It either passes the file content in the prompt for short documents\n",
    "    2. Performs a vector search for longer documents\n",
    "\n",
    "Retrieval currently optimizes for quality by adding all relevant content to the context of model calls. We plan to introduce other retrieval strategies to enable developers to choose a different tradeoff between retrieval quality and model usage cost.\n",
    "\n",
    "https://platform.openai.com/docs/assistants/tools/how-it-works\n",
    "\n",
    "![ALT TEXT](https://raw.githubusercontent.com/panaverse/learn-generative-ai/788f968387b0ad38bc379c3a4718400e8e42948d/03_chatgpt/07_assistants_knowledge_retrieval/ret.jpg)\n",
    "\n",
    "![ALT TEXT](https://raw.githubusercontent.com/panaverse/learn-generative-ai/788f968387b0ad38bc379c3a4718400e8e42948d/03_chatgpt/07_assistants_knowledge_retrieval/objects.jpeg)\n",
    "\n",
    "\n",
    "https://cobusgreyling.medium.com/openai-assistant-with-retriever-tool-08e9158ca900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv('C:/Code_Apps/Learn-Generative-AI/03_chatgpt/.env'))\n",
    "\n",
    "api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Upload the file and Create an Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.beta import Assistant\n",
    "\n",
    "#Upload a file with an \"assistants\" purpose\n",
    "file1 = client.files.create(\n",
    "    file=open(\"Muhammad_Talha_Khalid_013.pdf\",\"rb\"),\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "file2 = client.files.create(\n",
    "    file=open(\"fastapi-modern-python-web-development.pdf\",\"rb\"),\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "list([file1,file2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant : Assistant = client.beta.assistants.create(\n",
    "    name=\"File Searching\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\":\"retrieval\"}],\n",
    "    instructions=\"You are an expert teacher, understand concepts and explain it in simple words\",\n",
    "    file_ids=[file1.id,file2.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_6rjJIohXvSZowWkFZZvxFLH1',\n",
       " 'created_at': 1706980067,\n",
       " 'metadata': {},\n",
       " 'object': 'thread'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.types.beta.thread import Thread\n",
    "\n",
    "thread : Thread = client.beta.threads.create()\n",
    "\n",
    "dict(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Add Messages to Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreadMessage(id='msg_z1kWa32A4NxL4RIyhvl3VQOo', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are main teaching methods?'), type='text')], created_at=1706980732, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.types.beta.threads.thread_message import ThreadMessage\n",
    "\n",
    "message1 : ThreadMessage = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What are main teaching methods?\"\n",
    ")\n",
    "message1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreadMessage(id='msg_vCkGZWmk4xUjuscU68cAXQmR', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are multiple workers'), type='text')], created_at=1706980740, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message2 : ThreadMessage = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What are multiple workers\"\n",
    ")\n",
    "message2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run The Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(id='run_Wt5SIL72FDSaJJd8wzaGlrM5', assistant_id='asst_yCs8XcnbUZ2QUv5ozJ7iG4he', cancelled_at=None, completed_at=None, created_at=1706981016, expires_at=1706981616, failed_at=None, file_ids=['file-UcGhriFsd9Uoze7k9o5DhtkQ', 'file-afX2QALZrDqMP6bWENbCuVLd'], instructions='Try to answer in short way', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1', tools=[ToolAssistantToolsRetrieval(type='retrieval')], usage=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.types.beta.threads.run import Run\n",
    "\n",
    "run:Run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Try to answer in short way\"\n",
    ")\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Check The Run Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(id='run_Wt5SIL72FDSaJJd8wzaGlrM5', assistant_id='asst_yCs8XcnbUZ2QUv5ozJ7iG4he', cancelled_at=None, completed_at=None, created_at=1706981016, expires_at=1706981616, failed_at=None, file_ids=['file-UcGhriFsd9Uoze7k9o5DhtkQ', 'file-afX2QALZrDqMP6bWENbCuVLd'], instructions='Try to answer in short way', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1706981016, status='in_progress', thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1', tools=[ToolAssistantToolsRetrieval(type='retrieval')], usage=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.beta.threads.runs.retrieve(\n",
    "    run_id=run.id,\n",
    "    thread_id=thread.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Display The Assistant Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ThreadMessage(id='msg_AO5a4FjVRB8piR54efVlqajE', assistant_id='asst_yCs8XcnbUZ2QUv5ozJ7iG4he', content=[MessageContentText(text=Text(annotations=[], value='Multiple workers in the context of FastAPI and ASGI (Asynchronous Server Gateway Interface) refer to the ability to manage multiple worker processes to handle requests. FastAPI is based on ASGI, and it uses a special Uvicorn worker class that can be managed by Gunicorn to supervise multiple workers. Both Gunicorn and Uvicorn can start multiple worker processes, with Gunicorn typically needing 4-12 worker processes to handle hundreds or thousands of requests per second【18†source】.'), type='text')], created_at=1706981017, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_Wt5SIL72FDSaJJd8wzaGlrM5', thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_q0RLfJerbDDQU4VGKvZsrJjH', assistant_id='asst_yCs8XcnbUZ2QUv5ozJ7iG4he', content=[MessageContentText(text=Text(annotations=[], value='The main teaching methods include:\\n\\n1. Demonstration method: Involves the explanation and illustration of a concept or skill through performing a task, explaining the steps involved, and using visual aids to enhance understanding【9†source】.\\n\\n2. Project method: Involves students working on an extended in-depth project over a period, emphasizing hands-on experiential learning, research, problem-solving, and presentation of their projects【9†source】.\\n\\n3. Micro-teaching: Involves short focused teaching sessions in a controlled environment, allowing novice teachers to practice and refine their teaching skills, receive constructive feedback, and engage in self-reflection【9†source】.\\n\\nAs for \"multiple workers,\" I will look for relevant information.'), type='text')], created_at=1706980796, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_PUM3oJ7TlkqCqZjbOtgBTsuj', thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_vCkGZWmk4xUjuscU68cAXQmR', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are multiple workers'), type='text')], created_at=1706980740, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_z1kWa32A4NxL4RIyhvl3VQOo', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are main teaching methods?'), type='text')], created_at=1706980732, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_PuEAhx8obf8G64lgAy2NB9bj', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are multiple workers'), type='text')], created_at=1706980156, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_qin5dpPyzzWeSSWHesrjAbUp', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are main teaching methods?'), type='text')], created_at=1706980145, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: What are main teaching methods?\n",
      "user: What are multiple workers\n",
      "user: What are main teaching methods?\n",
      "user: What are multiple workers\n",
      "assistant: The main teaching methods include:\n",
      "\n",
      "1. Demonstration method: Involves the explanation and illustration of a concept or skill through performing a task, explaining the steps involved, and using visual aids to enhance understanding【9†source】.\n",
      "\n",
      "2. Project method: Involves students working on an extended in-depth project over a period, emphasizing hands-on experiential learning, research, problem-solving, and presentation of their projects【9†source】.\n",
      "\n",
      "3. Micro-teaching: Involves short focused teaching sessions in a controlled environment, allowing novice teachers to practice and refine their teaching skills, receive constructive feedback, and engage in self-reflection【9†source】.\n",
      "\n",
      "As for \"multiple workers,\" I will look for relevant information.\n",
      "assistant: Multiple workers in the context of FastAPI and ASGI (Asynchronous Server Gateway Interface) refer to the ability to manage multiple worker processes to handle requests. FastAPI is based on ASGI, and it uses a special Uvicorn worker class that can be managed by Gunicorn to supervise multiple workers. Both Gunicorn and Uvicorn can start multiple worker processes, with Gunicorn typically needing 4-12 worker processes to handle hundreds or thousands of requests per second【18†source】.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ThreadMessage(id='msg_AO5a4FjVRB8piR54efVlqajE', assistant_id='asst_yCs8XcnbUZ2QUv5ozJ7iG4he', content=[MessageContentText(text=Text(annotations=[], value='Multiple workers in the context of FastAPI and ASGI (Asynchronous Server Gateway Interface) refer to the ability to manage multiple worker processes to handle requests. FastAPI is based on ASGI, and it uses a special Uvicorn worker class that can be managed by Gunicorn to supervise multiple workers. Both Gunicorn and Uvicorn can start multiple worker processes, with Gunicorn typically needing 4-12 worker processes to handle hundreds or thousands of requests per second【18†source】.'), type='text')], created_at=1706981017, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_Wt5SIL72FDSaJJd8wzaGlrM5', thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_q0RLfJerbDDQU4VGKvZsrJjH', assistant_id='asst_yCs8XcnbUZ2QUv5ozJ7iG4he', content=[MessageContentText(text=Text(annotations=[], value='The main teaching methods include:\\n\\n1. Demonstration method: Involves the explanation and illustration of a concept or skill through performing a task, explaining the steps involved, and using visual aids to enhance understanding【9†source】.\\n\\n2. Project method: Involves students working on an extended in-depth project over a period, emphasizing hands-on experiential learning, research, problem-solving, and presentation of their projects【9†source】.\\n\\n3. Micro-teaching: Involves short focused teaching sessions in a controlled environment, allowing novice teachers to practice and refine their teaching skills, receive constructive feedback, and engage in self-reflection【9†source】.\\n\\nAs for \"multiple workers,\" I will look for relevant information.'), type='text')], created_at=1706980796, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_PUM3oJ7TlkqCqZjbOtgBTsuj', thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_vCkGZWmk4xUjuscU68cAXQmR', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are multiple workers'), type='text')], created_at=1706980740, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_z1kWa32A4NxL4RIyhvl3VQOo', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are main teaching methods?'), type='text')], created_at=1706980732, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_PuEAhx8obf8G64lgAy2NB9bj', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are multiple workers'), type='text')], created_at=1706980156, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1'),\n",
       " ThreadMessage(id='msg_qin5dpPyzzWeSSWHesrjAbUp', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What are main teaching methods?'), type='text')], created_at=1706980145, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_6rjJIohXvSZowWkFZZvxFLH1')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response : list[ThreadMessage] = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "display(list(response))\n",
    "\n",
    "for msg in reversed(response.data):\n",
    "    print(f\"{msg.role}: {msg.content[0].text.value}\")\n",
    "\n",
    "\n",
    "response.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
